{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>CS918 Assignment 1</h2>\n",
    "<h4>Faraz Taheri (1534783)</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Introduction</h2>\n",
    "<p>In this assignment, the news articles in SIGNAL NEWS1 corpus are processed to perform some natural language processing exercises.</p>\n",
    "<p><b>***The <i>signal-news1</i> folder must be located in the same directory as this file for the program to execute successfully.</b><p>\n",
    "<p>To run this program, the following libraries have to be imported. Some of the functinos are implemented using the NLTK library and it has to be installed separately.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "import math\n",
    "import nltk\n",
    "from nltk import trigrams\n",
    "nltk.data.path.append('/modules/cs918/nltk_data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>These global variables are used accross different sections of the program.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = []\n",
    "lemmatised = []  # All of lemmatised words\n",
    "lemmatised_first_16000 = []  # Lemmatised words for the first 16000 articles\n",
    "lemmatised_after_16000 = []  # Lemmatised words for the rest of the articles\n",
    "pos_words = {}  # Positive words for sentiment analysis\n",
    "neg_words = {}  # Negative words for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part A: Text preprocessing</h2>\n",
    "<h3>Data Sanitization</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The corpus used in this exercised is a file consisting of multiple json data. It has to be processed and sanitized. Each json data has multiple attributes such as <i>id, title, source, etc.</i>, but only the <i>content</i> field is used here.</p>\n",
    "<p>Different regex patterns are used to convert the data to the required format.</p>\n",
    "<p>Articles are stored in a list and each article is stored as a dictionary, and only the content field data is stored. Other fields of the news file can be stored as well.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('signal-news1/signal-news1.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        tmp_article = {'content': json.loads(line)['content']}\n",
    "        tmp_article['content'] = tmp_article['content'].lower()\n",
    "        # Remove Url\n",
    "        tmp_article['content'] = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', tmp_article['content'], flags=re.MULTILINE)\n",
    "        # Remove non-alphanumeric except spaces\n",
    "        tmp_article['content'] = re.sub(r'[^a-zA-Z\\d\\s:]', '', tmp_article['content'], flags=re.MULTILINE)\n",
    "        # Remove single characters\n",
    "        tmp_article['content'] = re.sub(r'\\b(\\w)\\b', '', tmp_article['content'], flags=re.MULTILINE)\n",
    "        # Remove single numbers\n",
    "        tmp_article['content'] = re.sub(r'\\b\\d+\\b', '', tmp_article['content'], flags=re.MULTILINE)\n",
    "        articles.append(tmp_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Lemmatisation</h3>\n",
    "<p>The NLTK package is used for lemmatising the text, using the WordNetLemmatizer function. The default POS tagging is used here.\n",
    "<p>For each article, all the words are lemmatised and stored as a dictionary in the <i>'lemmatised'</i> field of that article. The lemmatised words are used as keys, and their frequencies are the values.</p><p>There is also a global list of lemmatised words, consisting of the words in all the articles (in the first 16000 articles and the rest).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_count = 0\n",
    "for article in articles:\n",
    "    words = article['content'].split()\n",
    "    article['lemmatised'] = {}\n",
    "    articles_count += 1\n",
    "    for word in words:\n",
    "        lemm = nltk.stem.WordNetLemmatizer().lemmatize(word)  # Lemmatise the word\n",
    "        if articles_count <= 16000:\n",
    "            lemmatised_first_16000.append(lemm)\n",
    "        else:\n",
    "            lemmatised_after_16000.append(lemm)\n",
    "        if lemm not in article['lemmatised']:\n",
    "            article['lemmatised'][lemm] = 1\n",
    "        else:\n",
    "            article['lemmatised'][lemm] += 1\n",
    "lemmatised = lemmatised_first_16000 + lemmatised_after_16000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part B: N-grams</h2>\n",
    "<h3>Number of tokens and vocabulary size</h3>\n",
    "<p>To calculate the number of tokens (N), the size of the lemmatised words list is used.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens (N):  5701427\n"
     ]
    }
   ],
   "source": [
    "print('Number of Tokens (N): ', len(lemmatised))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>To calculate the vocabulary size (V), the size of unique lemmatised words is used. The set function creates an unordered collection of unique elements in the lemmatised words list.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size (V):  128800\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary Size (V): ', len(set(lemmatised)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Top 25 trigrams based on the number of occurrences</h3>\n",
    "<p>Using the NLTK's <i>trigrams</i> function (or <i>ngrams(3)</i>), a list of trigrams in the text is generated. By calculating their frequency and sorting them, the top 25 are selected. This could alternativetly be achieved by using NLTK's <i>FreqDist()</i>and<i>most_common(25)</i> function.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 25 trigrams:  [('one', 'of', 'the'), ('on', 'share', 'of'), ('on', 'the', 'stock'), ('a', 'well', 'a'), ('in', 'research', 'report'), ('in', 'research', 'note'), ('the', 'united', 'state'), ('for', 'the', 'quarter'), ('average', 'price', 'of'), ('research', 'report', 'on'), ('research', 'note', 'on'), ('share', 'of', 'the'), ('the', 'end', 'of'), ('in', 'report', 'on'), ('earnings', 'per', 'share'), ('cell', 'phone', 'plan'), ('phone', 'plan', 'detail'), ('according', 'to', 'the'), ('of', 'the', 'company'), ('buy', 'rating', 'to'), ('appeared', 'first', 'on'), ('moving', 'average', 'price'), ('day', 'moving', 'average'), ('price', 'target', 'on'), ('part', 'of', 'the')]\n"
     ]
    }
   ],
   "source": [
    "# Calculate top 25 trigrams\n",
    "tri = trigrams(lemmatised)\n",
    "#fd = nltk.FreqDist(tri)\n",
    "#top = fd.most_common(25)\n",
    "dist = {}\n",
    "for g in tri:\n",
    "    if g in dist:\n",
    "        dist[g] += 1\n",
    "    else:\n",
    "        dist[g] = 1\n",
    "top25 = sorted(dist.items(), key=lambda kv: kv[1], reverse=True)[:25]  # Sort trigrams and pick top 25\n",
    "print('Top 25 trigrams: ', [g[0] for g in top25])  # Select the key (Trigram tuple). The frequency is g[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Sentiment analysis</h3>\n",
    "<p>The corpus includes a list of positive and negative words. These lists are loaded from a file and are used in order to count the number of positive and negative words in a given text.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load positive and negative words\n",
    "with open('signal-news1/opinion-lexicon-English/positive-words.txt') as f:\n",
    "    for line in f:\n",
    "        pos_words[line.strip()] = 1\n",
    "\n",
    "with open('signal-news1/opinion-lexicon-English/negative-words.txt') as f:\n",
    "    for line in f:\n",
    "        neg_words[line.strip()] = -1\n",
    "\n",
    "\n",
    "# Count positive words in a lemmatised word set\n",
    "def count_pos_words(words_set):\n",
    "    num = 0\n",
    "    for w in words_set:\n",
    "        if w in pos_words:\n",
    "            num += words_set[w]\n",
    "    return num\n",
    "\n",
    "\n",
    "# Count negative words in a lemmatised word set \n",
    "def count_neg_words(words_set):\n",
    "    num = 0\n",
    "    for w in words_set:\n",
    "        if w in neg_words:\n",
    "            num += words_set[w]\n",
    "    return num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>For each article, the number of positive and negative lemmatised words is counted and based on the results, the positivity or negativity of each article is determined. The sum of positive and negative words throughout the whole document is also calculated.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive words:  170251\n",
      "Number of negative words:  129278\n",
      "Number of positive articles:  10816\n",
      "Number of negative articles:  6385\n"
     ]
    }
   ],
   "source": [
    "total_pos_articles = 0\n",
    "total_neg_articles = 0\n",
    "total_pos_words = 0\n",
    "total_neg_words = 0\n",
    "\n",
    "for article in articles:\n",
    "    num_pos_words = count_pos_words(article['lemmatised'])\n",
    "    num_neg_words = count_neg_words(article['lemmatised'])\n",
    "    total_pos_words += num_pos_words\n",
    "    total_neg_words += num_neg_words\n",
    "\n",
    "    if num_pos_words > num_neg_words:\n",
    "        total_pos_articles += 1\n",
    "    elif num_pos_words < num_neg_words:\n",
    "        total_neg_articles += 1\n",
    "\n",
    "print('Number of positive words: ', total_pos_words)\n",
    "print('Number of negative words: ', total_neg_words)\n",
    "print('Number of positive articles: ', total_pos_articles)\n",
    "print('Number of negative articles: ', total_neg_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part C: Language models</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Building a language model</h3>\n",
    "<p>To generate a 10-word sentence, a language model must be built. Using NLTK's trigrams function, a model is generated based on the first 160000 aricles. <br>The padding parameters allow us to use <i>None</i> in the trigrams and find the trigrams for starting and finishing sentences.<br>The <i>defaultdict</i> class from the <i>collections</i> pacakge is a type of dictionary that generates a dictionary item if the key does not exist when accessed.</p><p>The default value of 0.01 is because <i>log(0)</i> is illegal when calculating perplexity; it also provide slight smoothing.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a 10 word sentence\n",
    "model = defaultdict(lambda: defaultdict(lambda: 0.01))  # For smoothing. Also 0 cannot be used with math.log\n",
    "first_16000_trigrams = trigrams(lemmatised_first_16000, pad_right=True, pad_left=True)\n",
    "\n",
    "for w1, w2, w3 in first_16000_trigrams:\n",
    "    model[(w1, w2)][w3] += 1  # Count the appearance of the word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>For each pair of words, the probability of a following third word appearing must be calculated.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in model:\n",
    "    total_count = float(sum(model[pair].values()))\n",
    "    for w3 in model[pair]:\n",
    "        model[pair][w3] /= total_count  # Count the probability of the word appearing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Generating a 10-word sentence</h3>\n",
    "<p>Next step is to generate a 10 word sentence, with the first 2 words provided. At each step, the next word must be chosen in a way that has the highest probability of appearing, given the previous 2 words.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 10 word sentence: is this going to be the first time in the\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"is\", \"this\"]  # First 2 words of the sentence\n",
    "while len(sentence) < 10:\n",
    "    words = model.get(tuple(sentence[-2:]))  # Get the trigrams starting with the last pair of words\n",
    "    word = max(words.items(), key=operator.itemgetter(1))[0]  # Get the next word with maximum probability\n",
    "    sentence.append(word)\n",
    "print('Generated 10 word sentence:', ' '.join([w for w in sentence if w]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Calculating the perplexity</h3>\n",
    "<p>To calcuate the perplexity, the trigram model in the previous section is used as . The code below calculates the perpelixity using the chainrule for trigrams.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  42.77968412645628\n"
     ]
    }
   ],
   "source": [
    "# Calculate the perplexity\n",
    "P_log = float(0)  # Logs of probabilities\n",
    "N = 0  # Count\n",
    "for w1, w2, w3 in trigrams(lemmatised_after_16000, pad_left=True, pad_right=True):\n",
    "    N += 1\n",
    "    P_log += math.log2(model[(w1, w2)][w3])\n",
    "\n",
    "perplexity = pow(2, -P_log/N)\n",
    "print(\"Perplexity: \", perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>This is the end of the document.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
