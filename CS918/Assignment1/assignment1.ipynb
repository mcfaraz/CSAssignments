{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>CS918 Assignment 1</h2>\n",
    "<h4>Faraz Taheri (1534783)</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In this assignment, the news articles in SIGNAL NEWS1 corpus are processed to perform some natural language processing exercises.</p>\n",
    "<p>To run this program, the following libraries have to be imported.</p> Some of the functinos are implemented using the NLTK library and it has to be installed separately.</p>\n",
    "<b>***The <i>signal-news1</i> must be located in the same directory as this file for the progra</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk import trigrams\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>These global variables will be used accross different sections of the program.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = []\n",
    "lemmatised = []  # All of lemmatised words\n",
    "lemmatised_first_16000 = []  # Lemmatised words for the first 16000 articles\n",
    "lemmatised_after_16000 = []  # Lemmatised words for the rest of the articles\n",
    "pos_words = {}  # Positive words for sentiment analysis\n",
    "neg_words = {}  # Negative words for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part A: Text preprocessing</h2>\n",
    "<h3>Data Sanitization</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The corpus used in this exercised is a file consisting of multiple json data. It has to be processed and sanitized. Each json data has multiple attributes such as <i>id, title, source, etc.</i>, but only the <i>content</i> field is used here.</p>\n",
    "<p>Different regex patterns are used to convert the data to the required format.</p>\n",
    "<p>Articles are stored in a list and eech article is stored as a dictionary, and only the content field data is stored. Others fields of the news file can be stored as well.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('signal-news1/signal-news1.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        tmp_article = {'content': json.loads(line)['content']}\n",
    "        tmp_article['content'] = tmp_article['content'].lower()\n",
    "        # Remove Url\n",
    "        tmp_article['content'] = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', tmp_article['content'], flags=re.MULTILINE)\n",
    "        # Remove non-alphanumeric except spaces\n",
    "        tmp_article['content'] = re.sub(r'[^a-zA-Z\\d\\s:]', '', tmp_article['content'], flags=re.MULTILINE)\n",
    "        # Remove single characters\n",
    "        tmp_article['content'] = re.sub(r'\\b(\\w)\\b', '', tmp_article['content'], flags=re.MULTILINE)\n",
    "        # Remove single numbers\n",
    "        tmp_article['content'] = re.sub(r'\\b\\d+\\b', '', tmp_article['content'], flags=re.MULTILINE)\n",
    "        articles.append(tmp_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Lemmatisation</h3>\n",
    "<p>The NLTK package is used for lemmatising the text, using the WordNetLemmatizer function. The default POS tagging is used here.\n",
    "<p>For each article, all the words are lemmatised and stored as a dictionary in the <i>'lemmatised'</i> field of that article. The lemmatised words are used as keys, and their frequencies are the values.</p><p>There is also a global list of lemmatised words, consisting of the words in all the articles (in the first 16000 articles and the rest).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_count = 0\n",
    "for article in articles:\n",
    "    words = article['content'].split()\n",
    "    article['lemmatised'] = {}\n",
    "    articles_count += 1\n",
    "    for word in words:\n",
    "        lemm = nltk.stem.WordNetLemmatizer().lemmatize(word)  # Lemmatise the word\n",
    "        if articles_count <= 16000:\n",
    "            lemmatised_first_16000.append(lemm)\n",
    "        else:\n",
    "            lemmatised_after_16000.append(lemm)\n",
    "        if lemm not in article['lemmatised']:\n",
    "            article['lemmatised'][lemm] = 1\n",
    "        else:\n",
    "            article['lemmatised'][lemm] += 1\n",
    "lemmatised = lemmatised_first_16000 + lemmatised_after_16000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part B: N-grams</h2>\n",
    "<h3>Number of tokens and vocabulary size</h3>\n",
    "<p>To calculate the number of tokens (N), the size of the lemmatised words list is used.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens (N):  5701427\n"
     ]
    }
   ],
   "source": [
    "print('Number of Tokens (N): ', len(lemmatised))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>To calculate the vocabulary size (V), the size of unique lemmatised words is used. The set function creates an unordered collection of unique elements in the lemmatised words list.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size (V):  128800\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary Size (V): ', len(set(lemmatised)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Top 25 trigrams based on the number of occurrences</h3>\n",
    "<p>Using the NLTK package <i>trigrams</i> function (or <i>ngrams()</i>), a list of trigrams in the text is generated. By calculating their frequency and sorting them, the top 25 are selected. This can alternativetly done by using NLTK's <i>most_common(25)</i> function.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 25 trigrams:  [(('one', 'of', 'the'), 2434), (('on', 'share', 'of'), 2095), (('on', 'the', 'stock'), 1567), (('a', 'well', 'a'), 1424), (('in', 'research', 'report'), 1415), (('in', 'research', 'note'), 1373), (('the', 'united', 'state'), 1222), (('for', 'the', 'quarter'), 1221), (('average', 'price', 'of'), 1193), (('research', 'report', 'on'), 1177), (('research', 'note', 'on'), 1138), (('share', 'of', 'the'), 1132), (('the', 'end', 'of'), 1130), (('in', 'report', 'on'), 1124), (('earnings', 'per', 'share'), 1121), (('cell', 'phone', 'plan'), 1073), (('phone', 'plan', 'detail'), 1070), (('according', 'to', 'the'), 1064), (('of', 'the', 'company'), 1057), (('buy', 'rating', 'to'), 1016), (('appeared', 'first', 'on'), 995), (('moving', 'average', 'price'), 995), (('day', 'moving', 'average'), 993), (('price', 'target', 'on'), 981), (('part', 'of', 'the'), 935)]\n"
     ]
    }
   ],
   "source": [
    "# Calculate top 25 trigrams\n",
    "tri = trigrams(lemmatised)\n",
    "# top = tri.most_common(25)\n",
    "dist = {}\n",
    "for g in tri:\n",
    "    if g in dist:\n",
    "        dist[g] += 1\n",
    "    else:\n",
    "        dist[g] = 1\n",
    "top25 = sorted(dist.items(), key=lambda kv: kv[1], reverse=True)[:25]  # Sorting trigrams and selecting top 25\n",
    "print('Top 25 trigrams: ', [g[0] for g in top25])  # Selecting the key (Trigram tuple). The frequency is g[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Sentiment analysis</h3>\n",
    "<p>The corpus includes a list of positive and negative words. These lists are loaded from a file and are used in order the count the number of positive and negative words in a given text.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load positive and negative words\n",
    "with open('signal-news1/opinion-lexicon-English/positive-words.txt') as f:\n",
    "    for line in f:\n",
    "        pos_words[line.strip()] = 1\n",
    "\n",
    "with open('signal-news1/opinion-lexicon-English/negative-words.txt') as f:\n",
    "    for line in f:\n",
    "        neg_words[line.strip()] = -1\n",
    "\n",
    "\n",
    "# Count positive words in a word set\n",
    "def count_pos_words(words_set):\n",
    "    num = 0\n",
    "    for w in words_set:\n",
    "        if w in pos_words:\n",
    "            num += words_set[w]\n",
    "    return num\n",
    "\n",
    "\n",
    "# Count negative words in a word set \n",
    "def count_neg_words(words_set):\n",
    "    num = 0\n",
    "    for w in words_set:\n",
    "        if w in neg_words:\n",
    "            num += words_set[w]\n",
    "    return num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>For each article, the number of positive and negative lemmatised words is counted and based on the results, the positivity or negativity of each article is determined. The sum of positive and negative words throughout the whole document is also calculated.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive words:  170251\n",
      "Number of negative words:  129278\n",
      "Number of positive articles:  10816\n",
      "Number of negative articles:  6385\n"
     ]
    }
   ],
   "source": [
    "num_pos_articles = 0\n",
    "num_neg_articles = 0\n",
    "total_pos_words = 0\n",
    "total_neg_words = 0\n",
    "\n",
    "for article in articles:\n",
    "    num_pos_words = count_pos_words(article['lemmatised'])\n",
    "    num_neg_words = count_neg_words(article['lemmatised'])\n",
    "    total_pos_words += num_pos_words\n",
    "    total_neg_words += num_neg_words\n",
    "\n",
    "    if num_pos_words > num_neg_words:\n",
    "        num_pos_articles += 1\n",
    "    elif num_pos_words < num_neg_words:\n",
    "        num_neg_articles += 1\n",
    "\n",
    "print('Number of positive words: ', total_pos_words)\n",
    "print('Number of negative words: ', total_neg_words)\n",
    "print('Number of positive articles: ', num_pos_articles)\n",
    "print('Number of negative articles: ', num_neg_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part C: Language models</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Building a language model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
